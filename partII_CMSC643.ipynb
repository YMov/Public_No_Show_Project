{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Model Building\n",
    "\n",
    "Here you try your hand at model building to predict appointment no shows.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Package 'noshow_lib' now includes code to carry out preprocessing steps from part I. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noshow_lib.util as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it includes a dictionary used for configuring path and file names\n",
    "used through the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_path': 'data',\n",
       " 'raw_data_csv': 'KaggleV2-May-2016.csv',\n",
       " 'processed_data_path': 'processed_data',\n",
       " 'train_csv': 'train_set.csv',\n",
       " 'test_csv': 'test_set.csv',\n",
       " 'objstore_path': 'objects',\n",
       " 'feature_pipeline_file': 'feature_pipeline.pkl',\n",
       " 'labels_pipeline_file': 'labels_pipeline.pkl'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.file_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_pipeline_file`: file storing the preprocessing pipeline used for preparing the feature matrix\n",
    "\n",
    "`labels_pipeline_file`: file storing the preprocessing pipeline used for\n",
    "preparing labels\n",
    "\n",
    "`objstore_path`: directory to store python objects to disk\n",
    "\n",
    "`processed_data_path`: directory containing processed data\n",
    "\n",
    "`raw_data_csv`: name of the csv download from Kaggle\n",
    "\n",
    "`raw_data_path`: directory containing raw data\n",
    "\n",
    "`test_csv`: name of csv file containing test set\n",
    "\n",
    "`train_csv`: name of csv file containing train set\n",
    "\n",
    "You can change these paths and names to suit your project directory structure if you need so. E.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = utils.file_config\n",
    "#config['raw_data_path'] = \"some_other_directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create train test sets. Code is in file `noshow_lib/util.py` function `make_train_test_sets`. You\n",
    "can edit that function as needed to include your own part I code if you so desire. The result will be to \n",
    "create files `train_set.csv` and `test_set.csv` in your `processed_data` directory (unless you change any of the entries in the configuration directory as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/KaggleV2-May-2016.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ec33d71f0d66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mRUN_MAKE_TRAIN_TEST_FILES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mRUN_MAKE_TRAIN_TEST_FILES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_test_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\cmsc643_noshow-master\\cmsc643_noshow-master\\noshow_lib\\util.py\u001b[0m in \u001b[0;36mmake_train_test_sets\u001b[1;34m(config, test_size, random_state)\u001b[0m\n\u001b[0;32m     36\u001b[0m                          \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                          random_state = 1234):\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mkaggle_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_kaggle_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# rename some variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\cmsc643_noshow-master\\cmsc643_noshow-master\\noshow_lib\\util.py\u001b[0m in \u001b[0;36mread_kaggle_csv\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_kaggle_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'raw_data_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'raw_data_csv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_training_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\cmsc643_noshow-master\\cmsc643_noshow-master\\noshow_lib\\util.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(path, file)\u001b[0m\n\u001b[0;32m     19\u001b[0m     df = pd.read_csv(path + \"/\" + file,\n\u001b[0;32m     20\u001b[0m                      \u001b[0mparse_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'AppointmentDay'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ScheduledDay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                      dtype = {'Age': np.float64})\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'data/KaggleV2-May-2016.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# ONLY NEED TO RUN THIS STEP ONCE (switch this to True to run it)\n",
    "RUN_MAKE_TRAIN_TEST_FILES = True\n",
    "if RUN_MAKE_TRAIN_TEST_FILES:\n",
    "    utils.make_train_test_sets(config=file_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to fit the preprocessing pipelines. This is done in file `noshow_lib/preprocess.py`. Again you can edit code as needed in that file to incorporate your part I solution as you wish. The result will be to create files `feature_pipeline.pkl` and `labels_pipeline.pkl` containing the fit preprocessing pipelines we can then use to preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c14c60ac019b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnoshow_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ONLY NEED TO RUN THIS STEP ONCE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mRUN_FIT_PREPROCESSING\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mRUN_FIT_PREPROCESSING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\cmsc643_noshow-master\\cmsc643_noshow-master\\noshow_lib\\preprocess.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_pandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrameMapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnoshow_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_pandas'"
     ]
    }
   ],
   "source": [
    "import noshow_lib.preprocess as preprocess\n",
    "\n",
    "# ONLY NEED TO RUN THIS STEP ONCE\n",
    "RUN_FIT_PREPROCESSING = True\n",
    "if RUN_FIT_PREPROCESSING:\n",
    "    preprocess.fit_save_pipelines(config=file_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once we do that, we can get a training matrix and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90526, 101)\n",
      "(90526,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "Using `sklearn` fit:\n",
    "    - DecisionTree classifier\n",
    "    - RandomForest classifier\n",
    "    - Linear SVM classifier\n",
    "    - SVM with Radial Basis Kernel classifier\n",
    "    \n",
    "Use default parameters for now.\n",
    "Using 10-fold cross validation report both accuracy and AUC for each of the above four models.\n",
    "\n",
    "QUESTION: Should you use accuracy or AUC for this task as a performance metric?\n",
    "\n",
    "_ANSWER HERE_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8e0209a1b58f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mDT_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mDT_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Accuracy Eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "# build your models here\n",
    "\n",
    "#Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "DT_model.fit(train_X, train_y)\n",
    "\n",
    "#Accuracy Eval\n",
    "DT_model_accuracy = cross_val_score(DT_model, train_X, train_y, scoring=\"accuracy\", cv=10)\n",
    "DT_model_auc = cross_val_score(DT_model, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "print(DT_model_accuracy)\n",
    "print(DT_model_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators=100)\n",
    "RF_model.fit(train_X, train_y)\n",
    "\n",
    "#Accuracy Eval\n",
    "RF_model_accuracy = cross_val_score(RF_model, train_X, train_y, scoring=\"accuracy\", cv=10)\n",
    "RF_model_auc = cross_val_score(RF_model, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "print(RF_model_accuracy)\n",
    "print(RF_model_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM Classifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "LSVM_model = LinearSVC(kernel=\"linear\")\n",
    "LSVM_model.fit(train_X, train_y)\n",
    "\n",
    "#Accuracy Eval\n",
    "LSVM_model_accuracy = cross_val_score(LSVM_model, train_X, train_y, scoring=\"accuracy\", cv=10)\n",
    "LSVM_model_auc = cross_val_score(LSVM_model, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "print(LSVM_model_accuracy)\n",
    "print(LSVM_model_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBF SVM\n",
    "\n",
    "RBSVM_model = LinearSVC(kernel=\"rbf\")\n",
    "RBSVM_model.fit(train_X, train_y)\n",
    "\n",
    "#Accuracy Eval\n",
    "RBSVM_model_accuracy = cross_val_score(RBSVM_model, train_X, train_y, scoring=\"accuracy\", cv=10)\n",
    "RBSVM_model_auc = cross_val_score(RBSVM_model, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "print(RBSVM_model_accuracy)\n",
    "print(RBSVM_model_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "\n",
    "Based on the above, choose two methods and fit a tuned model:\n",
    "    - use 5-fold cross validation for model selection\n",
    "    - use 10-fold cross validation for model assessment (based on appropriate performance metric)\n",
    "\n",
    "Report estimated performance for both tuned classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune your models here\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'loss':['hinge', 'squared_hinge'],'max_features': [5, 10, 18, 25]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 30, 60,90,100], 'max_features': [5, 10, 18, 25, 30]}\n",
    "]\n",
    "\n",
    "\n",
    "# Ranndom_Forest Tuning\n",
    "\n",
    "grid_search = GridSearchCV(RF_model, param_grid=param_grid, scoring=\"roc_auc\", cv=5)\n",
    "grid_search.fit(train_X, train_y)\n",
    "print (\"Random_Forest: \", grid_search.best_params_)\n",
    "print(\"AUC score is {}\".format(grid_search.best_score_))\n",
    "\n",
    "pd.DataFrame(grid_search.cv_results_)\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "\n",
    "# Linear SVM Tuning with 10-fold\n",
    "LSVM_Score= cross_val_score(LinearSVC(C=10,loss='squared_hinge',verbose=1), train_X, train_y, cv=10, scoring='roc_auc')\n",
    "print(LSVM_Score)\n",
    "\n",
    "\n",
    "# RBF SVM Tuning\n",
    "\n",
    "vals = [0.0001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {'C': vals}\n",
    "RBSVM_GS = GridSearchCV(estimator=RBSVM_model, param_grid=param_grid, scoring=\"roc_auc\",cv=5)\n",
    "SVM_GS.fit(train_X, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initialize model parameters w and b\n",
    "# intializing to 0 is not a good idea\n",
    "# it should be a random vector see np.random.randn\n",
    "# YOU NEED TO IMPLEMENT THIS\n",
    "def _initialize_parameters(nfeatures):\n",
    "    w = np.full((nfeatures), 0.0)\n",
    "    b = np.full((1), 0.0)\n",
    "    return w, b\n",
    "\n",
    "# this is a vectorized version of positive_part operation\n",
    "# we can use this for hinge loss as post_part(1.0 - y*f)\n",
    "pos_part = np.vectorize(lambda u: u if u > 0. else 0.)\n",
    "\n",
    "# compute the value of the linear SVM objective function\n",
    "# given current signed distances, and parameter vector w\n",
    "def _get_objective(f, y, w, lam):\n",
    "    loss = np.sum(pos_part(1.0 - y*f))\n",
    "    penalty = lam * np.dot(w,w)\n",
    "    return loss + penalty\n",
    "\n",
    "# compute the signed distances\n",
    "# based on current model estimates\n",
    "# w and b\n",
    "# YOU NEED TO IMPLEMENT THIS\n",
    "def _get_signed_distances(X, w, b):\n",
    "    nobs = X.shape[0]\n",
    "    f = np.full(nobs, 0.0)\n",
    "    return f\n",
    "\n",
    "# compute gradients with respect to w and b\n",
    "# YOU NEEED TO IMPLEMENT THIS\n",
    "def _get_gradients(f, X, y, w, b, lam):\n",
    "    nfeatures = X.shape[1]\n",
    "    \n",
    "    gw = np.full(nfeatures, 0.0)\n",
    "    gb = np.full(1, 0.0)\n",
    "    return gw, gb\n",
    "\n",
    "# fit an SVM using gradient descent\n",
    "# X: matrix of feature values\n",
    "# y: labels (-1 or 1)\n",
    "# lam: penalty parameter lambda\n",
    "# n_iter: numer of iterations\n",
    "# eta: learning rate\n",
    "def fit_svm(X, y, lam, n_iter=100, eta=.4):\n",
    "    nexamples, nfeatures = X.shape\n",
    "    \n",
    "    w, b = _initialize_parameters(nfeatures)\n",
    "    \n",
    "    for k in range(n_iter):\n",
    "        f = _get_signed_distances(X, w, b)\n",
    "        \n",
    "        # print information and \n",
    "        # update the learning rate\n",
    "        if k % 10 == 0:\n",
    "            obj = _get_objective(f, y, w, lam)\n",
    "            eta = eta / 2.0\n",
    "            print(\"it: %d, obj %.2f\" % (k, obj))\n",
    "        \n",
    "        gw, gb = _get_gradients(f, X, y, w, b, lam)\n",
    "        w = w - eta * gw\n",
    "        b = b - eta * gb\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-98a233bb6a79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "w,b = fit_svm(train_X, train_y, 1.0, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
