{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Ensembles and Final Result\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "Train an AdaBoost classifier and compare its performance to results obtained in Part II using 10 fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noshow_lib.util as utils\n",
    "import noshow_lib.preprocess as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = utils.file_config\n",
    "file_config['raw_data_path'] = 'C:/Users/yazdan/Desktop/cmsc643_noshow-main/cmsc643_noshow-master'\n",
    "file_config['processed_data_path'] = \"Processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_path': 'C:/Users/yazdan/Desktop/cmsc643_noshow-main/cmsc643_noshow-master',\n",
       " 'raw_data_csv': 'KaggleV2-May-2016.csv',\n",
       " 'processed_data_path': 'Processed',\n",
       " 'train_csv': 'train_set.csv',\n",
       " 'test_csv': 'test_set.csv',\n",
       " 'objstore_path': 'objects',\n",
       " 'feature_pipeline_file': 'feature_pipeline.pkl',\n",
       " 'labels_pipeline_file': 'labels_pipeline.pkl'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.file_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79807799 0.79818845 0.7977466  0.79730476 0.79597923 0.79675246\n",
      " 0.79794521 0.79794521 0.79794521 0.7942996 ]\n",
      "[0.72655342 0.72599615 0.73008018 0.73593013 0.72343734 0.73179628\n",
      " 0.72824432 0.72474657 0.73303143 0.72261238]\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost code goes here\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "AB_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2))\n",
    "AB_model.fit(train_X, train_y)\n",
    "\n",
    "#Accuracy Eval\n",
    "AB_model_accuracy = cross_val_score(AB_model, train_X, train_y, scoring=\"accuracy\", cv=10)\n",
    "AB_model_auc = cross_val_score(AB_model, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "print(AB_model_accuracy)\n",
    "print(AB_model_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgBoost\n",
    "\n",
    "Train an xgBoost classifier and compare its performance to results in Part II using 10 fold CV. `sklearn` has a gradient boosting model included http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html which you can use. The `xgboost` package https://xgboost.readthedocs.io/en/latest/python/python_intro.htmlhas a wrapper you can use with sklearn as well https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn. The latter is more efficient at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79796752 0.79863029 0.7977466  0.79851983 0.79840937 0.79730476\n",
      " 0.79871852 0.79783473 0.79805568 0.79761379]\n",
      "[0.72981495 0.73023025 0.73317226 0.73895891 0.7267114  0.73508897\n",
      " 0.73467984 0.72839343 0.74008282 0.72581275]\n"
     ]
    }
   ],
   "source": [
    "# xgboost code here\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "XB_model = GradientBoostingClassifier()\n",
    "XB_model.fit(train_X, train_y)\n",
    "\n",
    "#Accuracy Eval\n",
    "XB_model_accuracy = cross_val_score(XB_model, train_X, train_y, scoring=\"accuracy\", cv=10)\n",
    "XB_model_auc = cross_val_score(XB_model, train_X, train_y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "print(XB_model_accuracy)\n",
    "print(XB_model_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Choose a set of 5 or so classifiers. Write a function that trains an ensemble using stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_stack_ensemble(X, y):\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "    # create train/validation sets\n",
    "    # using StratifiedShuffleSplit\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1234)\n",
    "    \n",
    "    for tr_index, ts_index in splitter.split(X, y):\n",
    "        \n",
    "        X_tr = X[tr_index]\n",
    "        y_tr = y[tr_index]\n",
    "        X_ts = X[ts_index]\n",
    "        y_ts = y[ts_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifiers in ensemble using train set\n",
    "    \n",
    "    DT_model = DecisionTreeClassifier()\n",
    "    RF_model = RandomForestClassifier()\n",
    "    LSVM_model = LinearSVC()\n",
    "    AB_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2))\n",
    "    XB_model = XB_model = GradientBoostingClassifier()\n",
    "    \n",
    "    classifiers_list=[DT_model, RF_model, LSVM_model, AB_model, XB_model]\n",
    "    classifires_names=['DT_model', 'RF_model', 'LSVM_model', 'AB_model', 'XB_model']\n",
    "    \n",
    "    # Define Prediction Set\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(len(classifiers_list)):\n",
    "        \n",
    "        classifiers_list[i].fit(X_tr,y_tr)        \n",
    "        predictions.append(classifiers_list[i].predict(X_ts)) \n",
    "           \n",
    "    \n",
    "    # create new feature matrix for validation\n",
    "    # set by getting predictions from the ensemble\n",
    "    # classifiers\n",
    "    \n",
    "    \n",
    "    # train logistic regression classifier on\n",
    "    # new feature matrix\n",
    "    LR_model = LogisticRegression()\n",
    "    LR_model.fit(predictions, y_ts)\n",
    "    \n",
    "    \n",
    "    EN_accuracy = cross_val_score(LR_model, X_ts, y_ts, scoring=\"accuracy\", cv=10)\n",
    "    EN_model_auc = cross_val_score(LR_model, X_ts, y_ts, scoring=\"roc_auc\", cv=10)\n",
    "    \n",
    "   \n",
    "    # return all trained classifiers \n",
    "    return classifiers_list, LR_model, X_tr, y_tr, X_ts, y_ts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold cross validation to measure performance of your stacked classifier. See Part II solution to see how to roll your own sklearn classifier along with http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5, 18106]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-cdf2f22eca65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Running Stacking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrained_classifiers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mts_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts_y\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mbuild_stack_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-dd751bb7b54d>\u001b[0m in \u001b[0;36mbuild_stack_ensemble\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# new feature matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mLR_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mLR_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m-> 1216\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5, 18106]"
     ]
    }
   ],
   "source": [
    "#Running Stacking\n",
    "\n",
    "trained_classifiers, LR_model,train_X, train_y, test_X, test_y  = build_stack_ensemble(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "Choose a single model based on all previous project steps. Train this model on the complete training dataset and measure it's performance on the held out test set.\n",
    "\n",
    "Compare to the 10-fold CV estimate you got previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final result goes here\n",
    "\n",
    "\n",
    "#RBF SVM\n",
    "\n",
    "RBSVM_model = SVC(kernel=\"rbf\")\n",
    "RBSVM_model.fit(train_X, train_y)\n",
    "\n",
    "#Accuracy Eval\n",
    "RBSVM_model_accuracy = cross_val_score(RBSVM_model, test_X, test_y, scoring=\"accuracy\", cv=10)\n",
    "RBSVM_model_auc = cross_val_score(RBSVM_model, test_X, test_y, scoring=\"roc_auc\", cv=10)\n",
    "\n",
    "print(RBSVM_model_accuracy)\n",
    "print(RBSVM_model_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
